# Design Document: Research-Digest (v1.0)

## 1. System Overview & Philosophy

**Research-Digest** is a "Research Funnel" engine. Its primary goal is to maximize recall (find all potentially relevant papers) while maximizing precision (remove noise via LLM verification) and minimizing user effort (automated summaries).

### Key Design Decisions

1. **Core-CLI Separation:** The business logic resides in a `core` package. The CLI is merely a consumer of this package, enabling future web/API integration.
2. **Stateful Operation:** We use a local **SQLite** database. Research is continuous; we must track what has been seen to avoid duplicate processing and costs.
3. **LLM Agnostic:** A "Provider" pattern handles LLM interactions, allowing seamless switching between local models (Ollama/vLLM) and commercial APIs (OpenAI/Anthropic).
4. **"Generate & Verify" Querying:** Instead of using intermediate query builders, we leverage the LLM to write raw arXiv query strings and validate them against the API before execution.
5. **Lazy Content Loading:** The system primarily operates on Abstracts (fast/cheap). A `ContentProvider` interface is used so that full PDF parsing can be swapped in later without rewriting the main logic.

---

## 2. Quick Start & Tech Stack

We use **`uv`** for modern, fast Python package management.

```bash
# 1. Initialize project
uv init research-digest --lib
cd research-digest

# 2. Add dependencies
# - 'tenacity': for retrying flaky LLM/API calls
# - 'rich': for beautiful CLI output
uv add arxiv sqlalchemy pydantic typer jinja2 openai rich python-dotenv tenacity

```

---

## 3. Project Structure

We follow a `src`-based layout to keep the package clean.

```text
research-digest/
├── pyproject.toml           # Dependencies
├── config.yaml              # User config (API keys, throttling, paths)
└── src/
    └── research_digest/
        ├── __init__.py
        ├── main.py          # Entry point for CLI
        ├── database.py      # DB connection & session management
        ├── models.py        # SQLAlchemy ORM models
        ├── config.py        # Pydantic settings management
        │
        ├── llm/             # LLM Abstraction Layer
        │   ├── __init__.py
        │   ├── provider.py  # Abstract Base Class & Implementations
        │
        ├── core/            # Business Logic
        │   ├── ingest.py    # Arxiv API interactions & Deduplication
        │   ├── translate.py # NL -> Raw Query String (with Validation)
        │   ├── filter.py    # The "Accurate Filter" logic (Relevance)
        │   ├── digest.py    # Summarization logic
        │
        └── reports/         # Output Generation
            ├── generators.py # Markdown, HTML, Email logic
            └── templates/    # Jinja2 templates

```

---

## 4. Data Architecture

We use **SQLAlchemy** (ORM) with **SQLite**.

### `Topic`

- `id`: Integer, PK
- `name`: String (e.g., "Agentic Workflows")
- `description`: String (User's natural language intent)
- `raw_query`: String (The validated query string generated by LLM)
- `last_run_at`: DateTime (Timestamp of the newest paper seen for this topic)
- `active`: Boolean

### `Paper`

- `id`: String, PK (The **Base ID**, e.g., `2310.00012`. Remove `v1`/`v2` suffixes.)
- `version`: Integer (Current version number)
- `title`: String
- `authors`: JSON/String
- `published_at`: DateTime (Original publication)
- `updated_at`: DateTime (Last update from Arxiv)
- `abstract`: Text
- `pdf_url`: String
- `processed_full_text`: Boolean (Future-proofing)

### `PaperTopicLink`

- _Junction table needed because one paper can match multiple topics._
- `paper_id`: FK
- `topic_id`: FK
- `relevance_score`: Float (0.0 - 10.0)
- `is_relevant`: Boolean (The final decision from the LLM filter)
- `digest`: Text (Topic-specific summary)
- `tags`: JSON/String
- `created_at`: DateTime

---

## 5. Module Logic & Prompts

### 5.1 Topic Translation (`core/translate.py`)

We convert user intent into a raw query string. We use `tenacity` to retry if the generated query is syntactically invalid.

**LLM System Prompt:**

```text
You are an expert in the arXiv API query syntax.
Convert the user's research topic into a SINGLE line raw query string.

Syntax Rules:
- Fields: ti (Title), abs (Abstract), cat (Category), au (Author).
- Operators: AND, OR, ANDNOT.
- Grouping: Use parentheses (...) for logic.
- Exact phrases: Use double quotes "..." for multi-word terms.

Common Categories:
- cs.AI (Artificial Intelligence), cs.CL (Computation & Language), cs.LG (Machine Learning)
- cs.SE (Software Eng), stat.ML (Machine Learning), cs.CV (Computer Vision)

Example:
User: "Large language models for medical diagnosis"
Output: (ti:"large language model" OR abs:"large language model" OR ti:LLM) AND (ti:medical OR abs:diagnosis) AND (cat:cs.CL OR cat:cs.AI)

Return ONLY the query string. No markdown, no explanations.

```

**Validation Logic:**

```python
@retry(stop=stop_after_attempt(3), retry=retry_if_exception_type(QueryValidationError))
def generate_valid_query(topic_description: str, llm_provider):
    raw_query = llm_provider.generate(topic_description, system_prompt=SYSTEM_PROMPT)
    try:
        # Dry Run: Fetch 1 result to test syntax
        client = arxiv.Client()
        search = arxiv.Search(query=raw_query, max_results=1)
        next(client.results(search), None)
    except Exception as e:
        raise QueryValidationError(f"Arxiv API rejected query: {e}")
    return raw_query

```

### 5.2 Ingest & Deduplication (`core/ingest.py`)

- **Logic:**

1. Fetch papers using `arxiv.Search(query=topic.raw_query, sort_by=SubmittedDate)`.
2. **Date Gate:** Stop fetching as soon as `paper.published_at <= topic.last_run_at`.
3. **Zero Results Check:** If the API returns 0 papers _on the very first run_ (when `last_run_at` is null/old), return a specific status flag.
4. **Deduplication:** Check if `base_id` exists. If yes, check `updated_at`. If new version, update record; otherwise skip.

### 5.3 Accurate Filter (`core/filter.py`)

This is the "expensive" step (conceptually), but we use cheap models (GPT-4o-mini / Llama3).

- **Prompt:**
  > Context: The user is interested in "{topic_description}".
  > Paper: "{title}"
  > Abstract: "{abstract}"
  > Task: Determine if this paper is RELEVANT to the user's interest.
  >
  > 1. Rate relevance (0-10).
  > 2. Decision: Should this be included in a weekly digest? (true/false).
  > 3. Short reasoning (1 sentence).
  >    Return JSON.

### 5.4 Summarization (`core/digest.py`)

Run only on relevant papers.

- **Prompt:**
  > Summarize the following paper for a researcher.
  > Format: **TL;DR** (1 sentence), **Key Contribution**, **Methodology**, **Tags** (3-5 keywords).

---

## 6. CLI & Application Logic (`main.py`)

This module ties everything together, handling user interaction, configuration, and scheduling behavior.

### 6.1 Configuration (`config.yaml`)

```yaml
llm:
  provider: "openai" # or "ollama"
  model: "gpt-4o-mini"
  api_key: "sk-..."

app:
  throttling_delay: 3.0 # Seconds to wait between topics
  db_path: "research.db"
```

### 6.2 The Update Loop (with Throttling & Quiet Mode)

```python
import time
import typer
from rich.console import Console

app = typer.Typer()
console = Console()

@app.command()
def update(quiet: bool = typer.Option(False, "--quiet", "-q", help="Run without output for cron jobs")):
    """Main loop to fetch and process new papers."""
    topics = db.get_active_topics()

    for topic in topics:
        if not quiet:
            console.print(f"[bold blue]Processing Topic:[/bold blue] {topic.name}")

        # 1. Ingest
        new_papers, status = ingest_module.fetch_papers(topic)

        # 2. Zero Results Feedback
        if status == "ZERO_RESULTS" and not quiet:
            console.print(f"[yellow]Warning: 0 results found for '{topic.name}'. Check your query: {topic.raw_query}[/yellow]")
            continue

        # 3. Filter & Digest Loop
        for paper in new_papers:
            # ... run filter and digest logic ...
            pass

        # 4. Throttling
        delay = config.app.throttling_delay
        if not quiet:
            console.print(f"Sleeping {delay}s to respect API limits...")
        time.sleep(delay)

if __name__ == "__main__":
    app()

```

---

## 7. LLM Provider Implementation (`llm/provider.py`)

A simplified interface since we are generating raw strings/JSON.

```python
class LLMProvider(ABC):
    @abstractmethod
    def generate(self, prompt: str, system_prompt: str = None) -> str:
        pass

class OpenAIProvider(LLMProvider):
    # Implements using openai.Client
    pass

class OllamaProvider(LLMProvider):
    # Implements using requests.post to localhost:11434
    pass

```

---

## 8. Implementation Roadmap

### Phase 1: Foundation & Database

1. Run `uv init` and add dependencies.
2. Define `models.py`.
3. Implement `config.py` to load `config.yaml`.

### Phase 2: Connectors & Core Logic

1. Implement `LLMProvider` (start with OpenAI for reliability, then add Ollama).
2. Implement `translate.py` with the "Generate & Verify" loop.
3. Implement `ingest.py` (fetching papers, handling dates).
4. **Test:** Create a script to add a topic and verify it generates a valid query string and fetches papers.

### Phase 3: The CLI & Throttling

1. Implement `main.py` using `typer`.
2. Add the `update` command with the `--quiet` flag.
3. Integrate the `time.sleep` throttling logic reading from config.
4. Add the "Zero Results" warning.

### Phase 4: Reporting

1. Create `reports/templates/digest.md.j2`.
2. Implement `digest report` command to output Markdown files.

### Phase 5: Refinement

1. Add `OllamaProvider`.
2. Refine prompts based on result quality.
3. Set up a Cron job to test the `--quiet` flag in a real background run.
